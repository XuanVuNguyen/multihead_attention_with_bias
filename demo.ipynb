{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modules import MultiHeadAttentionWithBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layer\n",
    "layer = MultiHeadAttentionWithBias(\n",
    "    embed_dim=32, \n",
    "    num_heads=4,\n",
    "    proj_bias=False,\n",
    "    attention_dropout=0.1,\n",
    "    kv_dim=None, # specify this argument if the input dims of query and key are different\n",
    "    )\n",
    "# For demonstration purposes, we will run the layer in inference mode to avoid the effect of dropout. \n",
    "# When initiate this layer, the following line is not necessary.\n",
    "layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inputs\n",
    "query = torch.rand((5, 10, 32)) # (bsz, q_len, embed_dim)\n",
    "key = torch.rand((5, 15, 32)) # (bsz, k_len, embed_dim)\n",
    "attention_bias = torch.rand((5, 10, 15)) # (bsz, q_len, k_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use cases\n",
    "## Self-attention\n",
    "Self-attention is executed when both `key_value` and `keys_values_cache` are not provided (think of the self-attention in the Encoder of Transformer); or `key_value` is not provided and `update_cache` is set to `True` (think of the self-attention in the Decoder of Transformer). In the case of the latter, the projected query will be returned to be reused if `return_current_keys_values` is set to `True`. \n",
    "\n",
    "### Self-attention without caching projected query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "# Simplest usage\n",
    "hidden_query, = layer(query)\n",
    "print(hidden_query.shape) # (bsz, q_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_query shape:  torch.Size([5, 10, 32])\n",
      "attn_weight shape:  torch.Size([5, 4, 10, 10])\n",
      "Sum of attn_weight is one along the last dimension: \n",
      " tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Getting the attention weight as output\n",
    "hidden_query, attn_weight = layer(query, return_attn_weights=True)\n",
    "print(\"hidden_query shape: \", hidden_query.shape) # (bsz, q_len, embed_dim)\n",
    "print(\"attn_weight shape: \", attn_weight.shape) # (bsz, num_heads, q_len, q_len)\n",
    "print(\"Sum of attn_weight is one along the last dimension: \\n\", attn_weight.sum(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention with caching for auto-regressive decoder\n",
    "In the scenario of self-attention in Transformer Decoder during inference, the query is updated auto-regressively, and self-attention is performed at every time-step. To avoid the recalculation of the projection of the past queries, we cache it into `keys_values_cache`, and update it at each time-step by setting `update_keys_values_cache` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50, 32])\n",
      "torch.Size([5, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "# Running the layer for 5 iterations\n",
    "keys_values_cache = None # None initialization\n",
    "for _ in range(5):\n",
    "    hidden_query, keys_values_cache = layer(query, \n",
    "        keys_values_cache=keys_values_cache,\n",
    "        update_keys_values_cache=True,\n",
    "        return_current_keys_values=True)\n",
    "\n",
    "past_keys, past_values = keys_values_cache\n",
    "print(past_keys.shape) # (bsz, 5*q_len, embed_dim)\n",
    "print(past_values.shape) # (bsz, 5*q_len, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-attention\n",
    "### Cross-attention without caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "# Simplest usage\n",
    "hidden_query, = layer(query, key)\n",
    "print(hidden_query.shape) # (bsz, q_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_query shape:  torch.Size([5, 10, 32])\n",
      "attn-weight shape:  torch.Size([5, 4, 10, 15])\n",
      "Sum of attn_weight is one along the last dimension: \n",
      " tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Getting the attention weight as output\n",
    "hidden_query, attn_weight = layer(query, key, return_attn_weights=True)\n",
    "print(\"hidden_query shape: \", hidden_query.shape) # (bsz, q_len, embed_dim)\n",
    "print(\"attn-weight shape: \", attn_weight.shape) # (bsz, num_heads, q_len, k_len)\n",
    "print(\"Sum of attn_weight is one along the last dimension: \\n\", attn_weight.sum(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention with caching for auto-regressive decoder\n",
    "In this case, we want to perform attention from different queries to the same key (think of the cross-attention in the decoder of Transformer during inference). Therefore, to avoid recaculating the projection of the key, we cache it into `keys_values_cache`. Different from self-attention, we will not update `keys_values_cache`, since the key is fixed, by setting `update_keys_values_cache` to `False`. In such case, if both `key` and `keys_values_cache` are provided, `key` will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 15, 32])\n",
      "torch.Size([5, 15, 32])\n"
     ]
    }
   ],
   "source": [
    "# Running the layer for 5 iterations\n",
    "keys_values_cache = None # None initialization\n",
    "# Only at the first iteration, the :key: argument is used. Afterward, the layer uses :keys_values_cache: instead, and ignores :key:.\n",
    "for _ in range(5):\n",
    "    hidden_query, keys_values_cache = layer(query, key,\n",
    "        keys_values_cache=keys_values_cache,\n",
    "        # update_keys_values_cache=False,\n",
    "        return_current_keys_values=True)\n",
    "\n",
    "past_keys, past_values = keys_values_cache\n",
    "print(past_keys.shape) # (bsz, k_len, embed_dim)\n",
    "print(past_values.shape) # (bsz, k_len, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including attention bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "hidden_query, = layer(query, key,\n",
    "    attention_bias=attention_bias)\n",
    "print(hidden_query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last five columns and the last three rows are masked: \n",
      " tensor([[0.0591, 0.1158, 0.0931, 0.1535, 0.1061, 0.0920, 0.1145, 0.0699, 0.1090,\n",
      "         0.0871, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0962, 0.0664, 0.1532, 0.0722, 0.0799, 0.0647, 0.0907, 0.1626, 0.1158,\n",
      "         0.0982, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0832, 0.1229, 0.1075, 0.1069, 0.0832, 0.0537, 0.1680, 0.0825, 0.0646,\n",
      "         0.1275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0752, 0.0747, 0.0772, 0.1410, 0.1029, 0.0584, 0.1310, 0.1429, 0.0967,\n",
      "         0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1234, 0.0695, 0.0999, 0.0698, 0.0975, 0.0803, 0.1269, 0.1555, 0.0720,\n",
      "         0.1051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0684, 0.1184, 0.0678, 0.1099, 0.0745, 0.1369, 0.0762, 0.0945, 0.1356,\n",
      "         0.1178, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0686, 0.0740, 0.1078, 0.1014, 0.1097, 0.0935, 0.1322, 0.1344, 0.0752,\n",
      "         0.1032, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "            nan,    nan,    nan,    nan,    nan,    nan],\n",
      "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "            nan,    nan,    nan,    nan,    nan,    nan],\n",
      "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "            nan,    nan,    nan,    nan,    nan,    nan]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# define a dummy attention mask. Let's say we want to mask out the last 3 tokens of query and last 5 tokens of key for every batch\n",
    "attention_mask = torch.zeros((5, 10, 15))\n",
    "for i in range(7):\n",
    "    for j in range(10):\n",
    "        attention_mask[:, i, j] = torch.ones((5))\n",
    "hidden_query, attn_weight = layer(query, key,\n",
    "    attention_bias=attention_bias,\n",
    "    attention_mask=attention_mask,\n",
    "    return_attn_weights=True)\n",
    "\n",
    "print(\"The last five columns and the last three rows are masked: \\n\", attn_weight[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e573ebb619b7fef874a85c8cf6bf9eab1ff93eb6b523008bbe22c110eb230c36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('huggingface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
