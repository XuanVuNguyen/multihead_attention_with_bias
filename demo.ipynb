{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modules import MultiHeadAttentionWithBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layer\n",
    "layer = MultiHeadAttentionWithBias(\n",
    "    embed_dim=32, \n",
    "    num_heads=4,\n",
    "    proj_bias=False,\n",
    "    attention_dropout=0.1,\n",
    "    kv_dim=None, # specify this argument if the input dims of query and key are different\n",
    "    )\n",
    "# For demonstration purposes, we will run the layer in inference mode to avoid the effect of dropout. \n",
    "# When initiate this layer, the following line is not necessary.\n",
    "layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inputs\n",
    "query = torch.rand((5, 10, 32)) # (bsz, q_len, embed_dim)\n",
    "key = torch.rand((5, 15, 32)) # (bsz, k_len, embed_dim)\n",
    "attention_bias = torch.rand((5, 10, 15)) # (bsz, q_len, k_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use cases\n",
    "## Self-attention\n",
    "Self-attention is executed when both `key_value` and `past_keys_values` are not provided (think of the self-attention in the Encoder of Transformer); or `key_value` is not provided and `is_autoregressive` is set to `True` (think of the self-attention in the Decoder of Transformer). In the case of the latter, the projected query will be returned to be reused if `return_current_keys_values` is set to `True`. \n",
    "\n",
    "### Self-attention without caching projected query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "# Simplest usage\n",
    "hidden_query, = layer(query)\n",
    "print(hidden_query.shape) # (bsz, q_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_query shape:  torch.Size([5, 10, 32])\n",
      "attN-weight shape:  torch.Size([5, 4, 10, 10])\n",
      "Sum of attn_weight is one along the last dimension: \n",
      " tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Getting the attention weight as output\n",
    "hidden_query, attn_weight = layer(query, return_attn_weights=True)\n",
    "print(\"hidden_query shape: \", hidden_query.shape) # (bsz, q_len, embed_dim)\n",
    "print(\"attn-weight shape: \", attn_weight.shape) # (bsz, num_heads, q_len, q_len)\n",
    "print(\"Sum of attn_weight is one along the last dimension: \\n\", attn_weight.sum(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention with caching for auto-regressive decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50, 32])\n",
      "torch.Size([5, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "# Running the layer for 5 iteration\n",
    "past_keys_values = None # None initialization\n",
    "for _ in range(5):\n",
    "    hidden_query, past_keys_values = layer(query, \n",
    "        past_keys_values=past_keys_values,\n",
    "        is_autoregressive=True,\n",
    "        return_current_keys_values=True)\n",
    "\n",
    "past_keys, past_values = past_keys_values\n",
    "print(past_keys.shape) # (bsz, 5*q_len, embed_dim)\n",
    "print(past_values.shape) # (bsz, 5*q_len, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-attention\n",
    "### Cross-attention without caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "# Simplest usage\n",
    "hidden_query, = layer(query, key)\n",
    "print(hidden_query.shape) # (bsz, q_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_query shape:  torch.Size([5, 10, 32])\n",
      "attn-weight shape:  torch.Size([5, 4, 10, 15])\n",
      "Sum of attn_weight is one along the last dimension: \n",
      " tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "          1.0000, 1.0000]]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Getting the attention weight as output\n",
    "hidden_query, attn_weight = layer(query, key, return_attn_weights=True)\n",
    "print(\"hidden_query shape: \", hidden_query.shape) # (bsz, q_len, embed_dim)\n",
    "print(\"attn-weight shape: \", attn_weight.shape) # (bsz, num_heads, q_len, k_len)\n",
    "print(\"Sum of attn_weight is one along the last dimension: \\n\", attn_weight.sum(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention with caching for auto-regressive decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 75, 32])\n",
      "torch.Size([5, 75, 32])\n"
     ]
    }
   ],
   "source": [
    "# Running the layer for 5 iteration\n",
    "past_keys_values = None # None initialization\n",
    "for _ in range(5):\n",
    "    hidden_query, past_keys_values = layer(query, key,\n",
    "        past_keys_values=past_keys_values,\n",
    "        is_autoregressive=True,\n",
    "        return_current_keys_values=True)\n",
    "\n",
    "past_keys, past_values = past_keys_values\n",
    "print(past_keys.shape) # (bsz, 5*k_len, embed_dim)\n",
    "print(past_values.shape) # (bsz, 5*k_len, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including attention bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "hidden_query, = layer(query, key,\n",
    "    attention_bias=attention_bias)\n",
    "print(hidden_query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last five columns and the last three rows are masked: \n",
      " tensor([[0.1614, 0.0603, 0.0999, 0.1353, 0.0606, 0.1235, 0.0741, 0.0832, 0.0803,\n",
      "         0.1214, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1369, 0.0959, 0.1065, 0.1270, 0.0598, 0.1096, 0.0756, 0.1253, 0.0661,\n",
      "         0.0973, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0844, 0.0825, 0.1149, 0.0698, 0.1559, 0.1376, 0.0790, 0.0837, 0.0958,\n",
      "         0.0964, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0806, 0.1615, 0.0681, 0.0829, 0.1467, 0.0608, 0.1125, 0.0659, 0.1229,\n",
      "         0.0981, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0829, 0.0906, 0.1431, 0.1379, 0.0601, 0.0558, 0.1146, 0.0677, 0.0878,\n",
      "         0.1597, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0995, 0.1261, 0.1343, 0.0611, 0.0678, 0.1146, 0.0955, 0.1047, 0.1005,\n",
      "         0.0959, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1163, 0.1233, 0.0981, 0.0767, 0.1030, 0.1043, 0.1125, 0.0504, 0.1219,\n",
      "         0.0935, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "            nan,    nan,    nan,    nan,    nan,    nan],\n",
      "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "            nan,    nan,    nan,    nan,    nan,    nan],\n",
      "        [   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "            nan,    nan,    nan,    nan,    nan,    nan]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# define a dummy attention mask. Let's say we want to mask out the last 3 tokens of query and last 5 tokens of key for every batch\n",
    "attention_mask = torch.zeros((5, 10, 15))\n",
    "for i in range(7):\n",
    "    for j in range(10):\n",
    "        attention_mask[:, i, j] = torch.ones((5))\n",
    "hidden_query, attn_weight = layer(query, key,\n",
    "    attention_bias=attention_bias,\n",
    "    attention_mask=attention_mask,\n",
    "    return_attn_weights=True)\n",
    "\n",
    "print(\"The last five columns and the last three rows are masked: \\n\", attn_weight[0, 0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e573ebb619b7fef874a85c8cf6bf9eab1ff93eb6b523008bbe22c110eb230c36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('huggingface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
